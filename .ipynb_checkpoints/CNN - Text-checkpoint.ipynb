{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTrain convolutional network for sentiment analysis. Based on\\n\"Convolutional Neural Networks for Sentence Classification\" by Yoon Kim\\nhttp://arxiv.org/pdf/1408.5882v2.pdf\\n\\nFor \\'CNN-non-static\\' gets to 82.1% after 61 epochs with following settings:\\nembedding_dim = 20          \\nfilter_sizes = (3, 4)\\nnum_filters = 3\\ndropout_prob = (0.7, 0.8)\\nhidden_dims = 100\\n\\nFor \\'CNN-rand\\' gets to 78-79% after 7-8 epochs with following settings:\\nembedding_dim = 20          \\nfilter_sizes = (3, 4)\\nnum_filters = 150\\ndropout_prob = (0.25, 0.5)\\nhidden_dims = 150\\n\\nFor \\'CNN-static\\' gets to 75.4% after 7 epochs with following settings:\\nembedding_dim = 100          \\nfilter_sizes = (3, 4)\\nnum_filters = 150\\ndropout_prob = (0.25, 0.5)\\nhidden_dims = 150\\n\\n* it turns out that such a small data set as \"Movie reviews with one\\nsentence per review\"  (Pang and Lee, 2005) requires much smaller network\\nthan the one introduced in the original article:\\n- embedding dimension is only 20 (instead of 300; \\'CNN-static\\' still requires ~100)\\n- 2 filter sizes (instead of 3)\\n- higher dropout probabilities and\\n- 3 filters per filter size is enough for \\'CNN-non-static\\' (instead of 100)\\n- embedding initialization does not require prebuilt Google Word2Vec data.\\nTraining Word2Vec on the same \"Movie reviews\" data set is enough to \\nachieve performance reported in the article (81.6%)\\n\\n** Another distinct difference is sliding MaxPooling window of length=2\\ninstead of MaxPooling over whole feature map as in the article\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train convolutional network for sentiment analysis. Based on\n",
    "\"Convolutional Neural Networks for Sentence Classification\" by Yoon Kim\n",
    "http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "\n",
    "For 'CNN-non-static' gets to 82.1% after 61 epochs with following settings:\n",
    "embedding_dim = 20          \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 3\n",
    "dropout_prob = (0.7, 0.8)\n",
    "hidden_dims = 100\n",
    "\n",
    "For 'CNN-rand' gets to 78-79% after 7-8 epochs with following settings:\n",
    "embedding_dim = 20          \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 150\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 150\n",
    "\n",
    "For 'CNN-static' gets to 75.4% after 7 epochs with following settings:\n",
    "embedding_dim = 100          \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 150\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 150\n",
    "\n",
    "* it turns out that such a small data set as \"Movie reviews with one\n",
    "sentence per review\"  (Pang and Lee, 2005) requires much smaller network\n",
    "than the one introduced in the original article:\n",
    "- embedding dimension is only 20 (instead of 300; 'CNN-static' still requires ~100)\n",
    "- 2 filter sizes (instead of 3)\n",
    "- higher dropout probabilities and\n",
    "- 3 filters per filter size is enough for 'CNN-non-static' (instead of 100)\n",
    "- embedding initialization does not require prebuilt Google Word2Vec data.\n",
    "Training Word2Vec on the same \"Movie reviews\" data set is enough to \n",
    "achieve performance reported in the article (81.6%)\n",
    "\n",
    "** Another distinct difference is sliding MaxPooling window of length=2\n",
    "instead of MaxPooling over whole feature map as in the article\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import data_helpers\n",
    "from w2v import train_word2vec\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Merge, Convolution1D, MaxPooling1D\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model variation is CNN-rand\n"
     ]
    }
   ],
   "source": [
    "model_variation = 'CNN-rand'  #  CNN-rand | CNN-non-static | CNN-static\n",
    "print('Model variation is %s' % model_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "sequence_length = 56\n",
    "embedding_dim = 20          \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 150\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word2Vec parameters, see train_word2vec\n",
    "min_word_count = 1  # Minimum word count                        \n",
    "context = 10        # Context window size    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "x, y, vocabulary, vocabulary_inv = data_helpers.load_data()\n",
    "\n",
    "if model_variation=='CNN-non-static' or model_variation=='CNN-static':\n",
    "    embedding_weights = train_word2vec(x, vocabulary_inv, embedding_dim, min_word_count, context)\n",
    "    if model_variation=='CNN-static':\n",
    "        x = embedding_weights[0][x]\n",
    "elif model_variation=='CNN-rand':\n",
    "    embedding_weights = None\n",
    "else:\n",
    "    raise ValueError('Unknown model variation')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.append(x,y,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = 0.15,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = test[:,:56]\n",
    "Y_test = test[:,56:58]\n",
    "\n",
    "\n",
    "X_train = train[:,:56]\n",
    "Y_train = train[:,56:58]\n",
    "train_rows = np.random.randint(0,X_train.shape[0],2500)\n",
    "X_train = X_train[train_rows]\n",
    "Y_train = Y_train[train_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 18779\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    \n",
    "    global graph_in\n",
    "    global convs\n",
    "    \n",
    "    graph_in = Input(shape=(sequence_length, embedding_dim))\n",
    "    convs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Buliding the first layer (Convolution Layer) of the network\n",
    "def build_layer_1(filter_length):\n",
    "    \n",
    "   \n",
    "    conv = Convolution1D(nb_filter=num_filters,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(graph_in)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding a max pooling layer to the model(network)\n",
    "def add_max_pooling(conv):\n",
    "    \n",
    "    pool = MaxPooling1D(pool_length=2)(conv)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding a flattening layer to the model(network), before adding a dense layer\n",
    "def add_flatten(conv_or_pool):\n",
    "    \n",
    "    flatten = Flatten()(conv_or_pool)\n",
    "    return flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_sequential(graph):\n",
    "    \n",
    "    #main sequential model\n",
    "    model = Sequential()\n",
    "    if not model_variation=='CNN-static':\n",
    "        model.add(Embedding(len(vocabulary), embedding_dim, input_length=sequence_length,\n",
    "                        weights=embedding_weights))\n",
    "    model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n",
    "    model.add(graph)\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.Convolution 2.Flatten\n",
    "def one_layer_convolution():\n",
    "    \n",
    "    initialize()\n",
    "    \n",
    "    conv = build_layer_1(3)\n",
    "    flatten = add_flatten(conv)\n",
    "    \n",
    "    convs.append(flatten)\n",
    "    out = convs[0]\n",
    "\n",
    "    graph = Model(input=graph_in, output=out)\n",
    "    \n",
    "    model = add_sequential(graph)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "          nb_epoch=num_epochs, validation_data=(X_test, Y_test))\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.Convolution 2.Max Pooling 3.Flatten\n",
    "def two_layer_convolution():\n",
    "    \n",
    "    initialize()\n",
    "    \n",
    "    conv = build_layer_1(3)\n",
    "    pool = add_max_pooling(conv)\n",
    "    flatten = add_flatten(pool)\n",
    "    \n",
    "    convs.append(flatten)\n",
    "    out = convs[0]\n",
    "\n",
    "    graph = Model(input=graph_in, output=out)\n",
    "    \n",
    "    model = add_sequential(graph)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "          nb_epoch=num_epochs, validation_data=(X_test, Y_test))\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.Convolution 2.Max Pooling 3.Flatten 4.Convolution 5.Flatten\n",
    "def three_layer_convolution():\n",
    "    \n",
    "    initialize()\n",
    "    \n",
    "    conv = build_layer_1(3)\n",
    "    pool = add_max_pooling(conv)\n",
    "    flatten = add_flatten(pool)\n",
    "    \n",
    "    convs.append(flatten)\n",
    "    \n",
    "    conv = build_layer_1(4)\n",
    "    flatten = add_flatten(conv)\n",
    "    \n",
    "    convs.append(flatten)\n",
    "    \n",
    "    if len(filter_sizes)>1:\n",
    "        out = Merge(mode='concat')(convs)\n",
    "    else:\n",
    "        out = convs[0]\n",
    "\n",
    "    graph = Model(input=graph_in, output=out)\n",
    "    \n",
    "    model = add_sequential(graph)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "          nb_epoch=10, validation_data=(X_test, Y_test))\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.Convolution 2.Max Pooling 3.Flatten 4.Convolution 5.Max Pooling 6.Flatten\n",
    "def four_layer_convolution():\n",
    "    \n",
    "    initialize()\n",
    "    \n",
    "    conv = build_layer_1(3)\n",
    "    pool = add_max_pooling(conv)\n",
    "    flatten = add_flatten(pool)\n",
    "    \n",
    "    convs.append(flatten)\n",
    "    \n",
    "    conv = build_layer_1(4)\n",
    "    pool = add_max_pooling(conv)\n",
    "    flatten = add_flatten(pool)\n",
    "    \n",
    "    convs.append(flatten)\n",
    "    \n",
    "    if len(filter_sizes)>1:\n",
    "        out = Merge(mode='concat')(convs)\n",
    "    else:\n",
    "        out = convs[0]\n",
    "\n",
    "    graph = Model(input=graph_in, output=out)\n",
    "    \n",
    "    model = add_sequential(graph)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "          nb_epoch=num_epochs, validation_data=(X_test, Y_test))\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 1600 samples\n",
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 26s - loss: 0.6936 - acc: 0.4992 - val_loss: 0.6938 - val_acc: 0.4769\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 26s - loss: 0.6930 - acc: 0.5152 - val_loss: 0.6926 - val_acc: 0.5231\n",
      "Test score: 0.692598519325\n",
      "Test accuracy: 0.523125\n",
      "CPU times: user 1min 6s, sys: 54.4 s, total: 2min\n",
      "Wall time: 59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#1.Convolution 2.Flatten\n",
    "one_layer_convolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 1600 samples\n",
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 21s - loss: 0.6934 - acc: 0.4920 - val_loss: 0.6931 - val_acc: 0.5081\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 21s - loss: 0.6930 - acc: 0.5072 - val_loss: 0.6923 - val_acc: 0.5231\n",
      "Test score: 0.692291852236\n",
      "Test accuracy: 0.523125\n",
      "CPU times: user 47.5 s, sys: 396 ms, total: 47.9 s\n",
      "Wall time: 48.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#1.Convolution 2.Max Pooling 3.Flatten\n",
    "two_layer_convolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 1600 samples\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 53s - loss: 0.6937 - acc: 0.5004 - val_loss: 0.6941 - val_acc: 0.4769\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 50s - loss: 0.6929 - acc: 0.5132 - val_loss: 0.6928 - val_acc: 0.5344\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 53s - loss: 0.6920 - acc: 0.5340 - val_loss: 0.6951 - val_acc: 0.4769\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 52s - loss: 0.6899 - acc: 0.5504 - val_loss: 0.6918 - val_acc: 0.5031\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 58s - loss: 0.6803 - acc: 0.6160 - val_loss: 0.6800 - val_acc: 0.6056\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 57s - loss: 0.6298 - acc: 0.6988 - val_loss: 0.6862 - val_acc: 0.5775\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 53s - loss: 0.5074 - acc: 0.7752 - val_loss: 0.6048 - val_acc: 0.6713\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 56s - loss: 0.3639 - acc: 0.8488 - val_loss: 0.6663 - val_acc: 0.6769\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 54s - loss: 0.2595 - acc: 0.8996 - val_loss: 0.6290 - val_acc: 0.6944\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 56s - loss: 0.1861 - acc: 0.9296 - val_loss: 0.7777 - val_acc: 0.6769\n",
      "Test score: 0.777654964328\n",
      "Test accuracy: 0.676875\n",
      "CPU times: user 9min 49s, sys: 4min 19s, total: 14min 9s\n",
      "Wall time: 9min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#1.Convolution 2.Max Pooling 3.Flatten 4.Convolution 5.Flatten\n",
    "three_layer_convolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 1600 samples\n",
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 50s - loss: 0.6937 - acc: 0.4976 - val_loss: 0.6927 - val_acc: 0.5231\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 51s - loss: 0.6929 - acc: 0.5148 - val_loss: 0.6926 - val_acc: 0.5231\n",
      "Test score: 0.692577679157\n",
      "Test accuracy: 0.523125\n",
      "CPU times: user 1min 58s, sys: 54.6 s, total: 2min 53s\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#1.Convolution 2.Max Pooling 3.Flatten 4.Convolution 5.Max Pooling 6.Flatten\n",
    "four_layer_convolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
