{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following is done to tokenize the tweets into its appropriate form\n",
    "#In particular, we try to capture some emoticons, HTML tags, Twitter @usernames (@-mentions), Twitter #hashtags, \n",
    "#URLs, numbers, words with and without dashes and apostrophes\n",
    "\n",
    "#Source : https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/\n",
    "class ParseTweets():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.emoticons_str = r\"\"\"\n",
    "        (?:\n",
    "            [:=;] # Eyes\n",
    "            [oO\\-]? # Nose (optional)\n",
    "            [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "        )\"\"\"\n",
    "        self.regex_str = [\n",
    "        self.emoticons_str,\n",
    "        r'<[^>]+>', # HTML tags\n",
    "        r'(?:@[\\w_]+)', # @-mentions\n",
    "        r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "        r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "\n",
    "        r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "        r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "        r'(?:[\\w_]+)', # other words\n",
    "        r'(?:\\S)' # anything else\n",
    "        ]\n",
    "        \n",
    "        self.tokens_re = re.compile(r'(' + '|'.join(self.regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "        self.emoticon_re = re.compile(r'^' + self.emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return self.tokens_re.findall(text)\n",
    "    \n",
    "    def preprocess(self, text, lowercase = False):\n",
    "        tokens = self.tokenize(text)\n",
    "        if lowercase:\n",
    "            tokens = [token if self.emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    def parse_tweets(self, filename, lowercase):\n",
    "        \n",
    "        text = ''\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "\n",
    "                tweet = json.loads(line) # load it as Python dict\n",
    "                tokens = self.preprocess(tweet['text'], lowercase)\n",
    "\n",
    "                for index,element in enumerate(tokens):\n",
    "\n",
    "                    #Removing '#' \n",
    "                    if('#' in element):\n",
    "\n",
    "                        del tokens[index]\n",
    "                        text = text + \"\"\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    #Removing the 'RT' tag\n",
    "                    elif('RT' in element):\n",
    "\n",
    "                        del tokens[index]\n",
    "                        text = text + \"\"\n",
    "                        continue\n",
    "\n",
    "                    #This character usually follows the 'RT' tag, so we remove it\n",
    "                    elif(':' in element):\n",
    "\n",
    "                        del tokens[index]\n",
    "                        text = text + \"\"\n",
    "                        continue\n",
    "\n",
    "                    text = text + \" \" + tokens[index]\n",
    "                    \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BuildCorpus():\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.sentences = list()\n",
    "        self.next_chars = list()\n",
    "        self.maxlen = 0\n",
    "        self.step = 0\n",
    "        self.text = text\n",
    "        \n",
    "    def build_sentences(self, maxlen = 40, step = 1):\n",
    "        # cut the text in semi-redundant sequences of maxlen characters\n",
    "        self.maxlen = maxlen\n",
    "        self.step = step\n",
    "        for i in range(0, len(self.text) - self.maxlen, self.step):\n",
    "            self.sentences.append(self.text[i: i + maxlen])\n",
    "            self.next_chars.append(self.text[i + maxlen])\n",
    "            \n",
    "    def vectorize_text(self):\n",
    "        \n",
    "        X = np.zeros((len(self.sentences), self.maxlen, len(self.chars)), dtype=np.bool)\n",
    "        Y = np.zeros((len(self.sentences), len(self.chars)), dtype=np.bool)\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            for t, char in enumerate(sentence):\n",
    "                X[i, t, self.char_indices[char]] = 1\n",
    "            Y[i, self.char_indices[self.next_chars[i]]] = 1\n",
    "            \n",
    "        return X, Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
